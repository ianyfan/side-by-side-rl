REINFORCE with Baseline (episodic), for estimating \( \pi_0 \approx \pi_*\)

Input: a differentiable policy parameterization \( \pi(a \mid s, \mathbf\theta) \)
Input: a differentiable state-value function parameterization \( \hat{v}(s, \mathbf{w}) \)
Algorithm parameters: step sizes \( \alpha^\mathbf\theta > 0 , \alpha^\mathbf{w} > 0 \)
Initialize policy parameter \( \mathbf\theta \in \mathbb{R}^{d'} \) and state-value weights \( \mathbf{w} \in \mathbb{R}^d \) (e.g., to \( \mathbf{0} \))

Loop forever (for each episode):
	Generate an episode \( S_0, A_0, R_1, \ldots, S_{T-1}, A_{T-1}, R_T \)
	Loop for each step of the episode \( t = 0, 1, \ldots, T - 1 \):
		\( G \leftarrow \sum_{k = t + 1}^{T}{\gamma^{k - t - 1} R_k} \)
		\( \delta \leftarrow G - \hat{v}(S_t, \mathbf{w}) \)
		\( \mathbf{w} \leftarrow \mathbf{w} + \alpha^\mathbf{w}\delta\nabla{\hat{v}(S_t, \mathbf{w})} \)
		\( \mathbf\theta \leftarrow \mathbf\theta + \alpha^\mathbf\theta \gamma^t \delta \nabla{\ln{\pi(A_t \mid S_t, \mathbf\theta)}} \)
