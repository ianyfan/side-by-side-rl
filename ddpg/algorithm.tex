DDPG algorithm

Randomly initialize critic network \( Q(s, a \mid \theta^Q) \) and actor \( \mu(s \mid \theta^\mu) \) with weights \( \theta^Q \) and \( \theta^\mu \).
Initialize target network \( Q' \) and \( \mu' \) with weights \( \theta^{Q'} \leftarrow \theta^Q \), \( \theta^{\mu'} \leftarrow \theta^\mu \)
Initialize replay buffer \( R \)
<b>for</b> episode \( = 1, M \) <b>do</b>
	Initialize a random process \( \mathcal{N} \) for action exploration
	Receive initial observation state \( s_1 \)
	<b>for</b> \( t = 1, T \) <b>do</b>
		Select action \( a_t = \mu(s_t \mid \theta^\mu) + \mathcal{N}_t \) according to the current policy and exploration noise
		Execute action \( a_t \) and observe reward and observe new state \( s_{t+1} \)
		Store transition \( (s_t, a_t, r_t, s_{t+1}) \) in \( R \)
		Sample a random minibatch of \( N \) transitions \( (s_i, a_i, r_i, s_{i+1}) \) from \( R \)
		Set \( y_i = r_i + \gamma Q'(s_{i+1}, \mu'(s_{i+1} \mid \theta^{\mu'}) \mid \theta^{Q'}) \)
		Update critic by minimizing the loss: \( L = \frac{1}{N}\sum_i(y_i - Q(s_i, a_i \mid \theta^Q))^2 \)
		Update the actor policy using the sampled policy gradient:
		\[ \nabla_{\theta^\mu}J \approx \frac{1}{N}\sum\limits_i{\nabla_a{Q(s, a \mid \theta^Q)\rvert_{s = s_i, a = \mu(s_i)}} \nabla_{\theta^\mu}{\mu(s \mid \theta^\mu)\rvert_{s_i}}} \]
		Update the target networks:
		\[ \theta^{Q'} \leftarrow \tau\theta^Q + (1 - \tau)\theta^{Q'} \]
		\[ \theta^{\mu'} \leftarrow \tau\theta^\mu + (1 - \tau)\theta^{\mu'} \]
