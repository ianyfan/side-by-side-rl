REINFORCE
<p>
  REINFORCE is an on-policy policy gradient learning algorithm, presented in
  <cite>
    <a href="//link.springer.com/article/10.1007/BF00992696">Simple statistical gradient-following algorithms for connectionist reinforcement learning</a>
  </cite>
  by Williams in 1992 (though the term "policy gradient algorithm" was only introduced in 2000 in
  <cite>
    <a href="//papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html">Policy Gradient Methods for Reinforcement Learning with Function Approximation</a>
  </cite>
  by Sutton et al.). The version shown here is from
  <cite>
    <a href="//incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction</a>
  </cite>
  (second edition) by Sutton &amp; Barto.
  <br>
  The algorithm simply learns a policy that directly maximizes the return of the policy, according to the policy gradient theorem. The algorithm is also augmented by comparing the return to a baseline, such as an estimate of the state value. This does not change the expected value of the update (leaving it unbiased), but can reduce variance, thus improve learning speed.
</p>
